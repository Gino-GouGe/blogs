[{"content":"Following the previous blog Leader Election, Log Replication is covered today. Once a leader has been elected, it begins servicing client requests. Each request contains a command to be executed by the replicated state machine.\nLog Replication Leader appends the command to its log and issues AppendEntries RPCs in parallel to all followers to replicate the entry. Leader will keep sending RPCs until all the followers catch up even though it replies to client after majority replication (e.g. 3/5). Example of logs are shown above Leader keeps track of the highest index it knows to be committed by making sure the entry has replicated on majority of the servers. This index is communicated to followers so that followers can find out. Server (both leader and follower) also have a state appliedAt index to track the highest index it has applied to the state machine. Upon discovering commit index is larger than appliedAt, a dedicated thread will start to apply the entry to state machine. Leader employs Consistency Check protocol to ensure the log is consistent across all servers. Whenever an inconsistent log entry detected, leader will force the follower\u0026rsquo;s logs to duplicate its own. Note: The main reason to separate appliedAt and commit index is to avoid waiting for time-consuming IO.\nSafety Safety put additional restrictions to guarantee the leader for any term contains all committed logs\nElection Restriction: A candidate\u0026rsquo;s log must at least up-to-date when it can be elected. Up-to-date is determined by comparing index and term of the last entry. If with different terms, the log with later term is up-to-date. If with same term, the longer is more up-to-date Append Entries Restriction: In addition to majority rule, leader can only commit entries in current term. In other words, leader CANNOT mark an entry as committed when the entry gets replicated to majority but in previous term (This sounds counter intuitive but Figure 8 in the paper explains the extreme scenario quite well and my explanation can be no match for that. So highly recommend to read the paper) Note: You may notice after introducing the Append Entries Restriction, the cluster can only make progress (i.e. increment commitIndex and apply the cmd to state machine) when there are new requests on current term. This is not optimal because the system better makes progress during idle time. One optimization (e.g. implemented by etcd) is to commit a no-op entry at current term immediately upon the election of a new leader so the system can still make progress even no client request\nImplementation AppendEntries RPC Upon receiving a request, leader will append the entry to its own log and issues AppendEntriesRPC. In order to know which entries to send to different followers, leader need to maintain a nextIndex array wrt each follower to track the next index haven\u0026rsquo;t been replicated.\nNote: There is a tricky part in follower appends it\u0026rsquo;s log. The rule according to the paper is \u0026ldquo;If an existing entry conflicts with a new one, delete the existing entry and all that follow it (ยง5.3)\u0026rdquo;. In other words, follower should not do anything when no conflicts. My original implementation rf.log = append(rf.log[:prevLogIndex], entries) is not safe b\\c of the example\nLeader issues 2 AppendEntriesRPC with entries [2,2], [2,2,3] in chronological order. The 2nd RPC gets to follower earlier b\\c of unstable network and has follower\u0026rsquo;s log to be [2,2,3], already updated. Trim from the prevLogIndex is definitely not safe b\\c it removes committed entry. Instead, the follower should scan the log and only delete when conflicts. When no conflict detected, the follower\u0026rsquo;s log should stay 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 // sender entry = Entry(term, command) logs.append(entry) func (rf *Raft) sendAppendEntries() { for follower in followers { /** AppendEntries RPC Section **/ nextIndex = nextIndex[followerId] request = AppendEntriesRequest(term, logs[nextIndex, len(logs) - 1]) go func() { ok = sendAppendEntriesRPC(request, reply) if append_entry_succeed { return } if discover_higher_term { update_term step_down_to_follower return } if receive_old_rpc { simply_discard return } } } } // receiver func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { if args.term \u0026lt; currentTerm { reply.success = false reply.term = currentTerm return } if found_conflicts { reply.success = false reply.term = currentTerm reply.conflictIndex = args.prevLogIndex return } // leader\u0026#39;s term is equal or higher append_the_log } CommitIndex To handle high concurrency, leader would just start a thread to append entries to followers without any synchronization among followers. To keep track the index replicated to every follower, leader also maintains a matchIndex array. After every update, it checks whether an index appears on majority of the followers and update the commit index.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // async for follower in followers { /** AppendEntries RPC Section **/ ... matchIndex[followerId] = leaderLastIndex /** Update matchIndex Section **/ for i = [leaderLastIndex, 0) { cnt = 0 for follower in followers { if i_replicated_on_follower { cnt += 1 } } if cnt \u0026gt; N/2 { commitIndex = i break } } } AppliedAt 1 2 3 4 5 6 7 8 func (rf *Raft) applyStateMachine() { for raft_is_not_killed { for appliedAt \u0026lt; commitIndex { apply_to_state_machine } time.Sleep(10 ms) } } Consistency Check AppendEntries RPCs will include the index and term of immediate preceding entry in leader\u0026rsquo;s log. If followers\u0026rsquo; log doesn\u0026rsquo;t match (either doesn\u0026rsquo;t have the index or term mismatch), the follower will refuse the entry and the leader will decrements to a previous entry and keeps trying until matches. Optimization: Above mentioned consistency check protocol backs off per entry. To reduce the number of RPCs, the protocol can be optimized to be per term by having follower sends more information. Concretely, upon a mis-matching detected Follower sends back conflictTerm, the term of mismatch index ConflictIndex, the 1st index of the conflict term Leader scan for its log If the log has conflictTerm, update nextIndex to the last index of conflictTerm If the log doesn\u0026rsquo;t have the term, update nextIndex to conflictIndex Leader and Follower may need special handling when follower\u0026rsquo;s log doesn\u0026rsquo;t have the index of leader\u0026rsquo;s log at all. My implementation sets conflict term to -1 and conflict index to len(log). On the leader side, set nextIndex to the conflictIndex\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // consistency check per entry for follower in followers { /** AppendEntries RPC Section **/ for state == leader { ... request = AppendEntriesRequest(term, logs[nextIndex, len(logs) - 1], prevLogIndex, prevLogTerm) ... /** Consistency Check Section **/ if conflictTerm == -1 { nextIndex[follower] = conflictIndex } if found_conflict { for log in logs reverse order { if conflictTerm exist { nextIndex[follower] = last_index_of_conflict_term } else { nextIndex[follower] = conflictIndex } } } } } // follower optimization func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { if prevLogIndex \u0026gt;= len(log) || log[prevLogIndex].Term != args.PrevLogTerm { if prevLogIndex \u0026gt;= len(rf.log) { reply.ConflictIndex = len(rf.log) reply.ConflictTerm = -1 } else { reply.ConflictTerm = log[prevLogIndex].Term reply.ConflictIndex = first_index_of_the_term } reply.Term = currentTerm reply.Succeeded = false return } } Safety Straight-forward implementation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // 1. Election Restriction func (rf *Raft) isCandidateUpToDate(RequestVoteRequest args) { if args.lastLogTerm \u0026lt; logs[-1].Term { reply.voteGranted = false return } if args.lastLogTerm \u0026gt; logs[-1].Term { reply.voteGranted = false return } if args.lastLogTerm == logs[-1].Index { reply.voteGranted = args.lastLogIndex \u0026gt;= len(logs) - 1 return } } func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { ... /** Additional Safety constrains to determine whether a candidate can be elected **/ if !isCandidateUpToDate(args) { reject_vote return } ... } // 2. Only commit entries in current term func (rf *Raft) sendAppendEntries() { ... /** Update matchIndex Section **/ for i = [leaderLastIndex, 0] { ... if cnt \u0026gt; N/2 \u0026amp;\u0026amp; logs[i].term == currentTerm { commitIndex = i break } } ... } // 3. Safety Optimization.Append a noop at current term as soon as the leader is elected ... state = leader logs.append(Entry(currentTerm, \u0026#39;noop\u0026#39;)) ... Ref Student Guide ","date":"2025-12-22T00:00:00Z","permalink":"https://Gino-GouGe.github.io/blogs/p/raft-log-replication/","title":"Raft Log Replication"},{"content":"This is the 1st blog about my progress in MIT 6.5840. Came up this idea to 1st summarize my understanding and implementation and 2nd share my learnings. I would cover Lab1, MapReduce and Lab2, simple KV server in future posts.\nAccording to the policy, my github repo is private and the code blocks below are pseudo-code explaining the algorithm.\nWhat is Raft? Raft is a consensus algorithm for managing replicated state machines. The key compositions of Raft are Leader Election, Log Replication and Safety. Safety puts additional constrains on Leader Election and Log Replication to guarantee the elected leader preserves all the committed states. Other than algorithm correctness and efficiency, one of the goals of Raft is understandability. i.e. to facilitate the development of intuitions that are essential to system builders. System builders can not only understand how it works but why it works to adjust/improve the algorithm for practical use-cases.\nA basic Raft requires only Leader Election and Log Replication. This blog is on Leader Election (w\\o Safety. Safety would be expanded later in Log Replication).\nLeader Election Raft operates on a cluster of odd numbered servers, e.g. a cluster of 3/5/7. 5 is a typical number and can tolerate 2 failures. The state machine and transition is depicted below\nEvery server can be in 3 states \u0026ldquo;follower\u0026rdquo;, \u0026ldquo;candidate\u0026rdquo;, \u0026ldquo;leader\u0026rdquo;. They start at \u0026ldquo;follower\u0026rdquo; state and wait for some time pass to start Leader Election. The preset timeout is called \u0026ldquo;Election Timeout\u0026rdquo; After randomized Election Timeout, one server switches state to \u0026ldquo;candidate\u0026rdquo;, bumps up its term, votes for itself and sends RequestVote RPCs to other servers in the cluster Candidate, the sender collects \u0026ldquo;# of votes got\u0026rdquo; and \u0026ldquo;# of requests sent\u0026rdquo; and can run into 4 scenarios below Gets majority votes (of a full cluster), steps up to \u0026ldquo;leader\u0026rdquo; and send HeartBeat (empty AppendEntries) immediately to establish authority Split vote, i.e. a candidate doesn\u0026rsquo;t get majority votes. It restarts Leader Election. Split votes usually caused by multiple servers start Leader Election simultaneously. To prevent constant split vote, a randomized Election Timeout is employed on each server Discovers a higher term, updates term to higher term and steps down to follower Upon Election Timeout, the candidate restarts Leader Election and the pending election attempt is discarded or ignored. Leader needs to send periodic HeartBeat to followers to maintain its authority Note: For the 3rd scenario in bullet 3, the primary goal to update term to higher term upon discovery is to bring eligible candidate up to the matching term with in-eligible (i.e. network partitioned) candidate with in-complete logs, where the in-eligible candidate just keeps bumping up its term. It will make more sense after Safety. A rough example,\nS0 is elected as leader and sending HeartBeat in term 1 S2 is partitioned, doesn\u0026rsquo;t receive HeartBeat, goes into leader election process S2 never successfully elected as a leader because of no majority votes from either S0 or S1. And it keeps triggering Leader Election with term bumping up. After some time the replicated logs are [0,1,1] (only term), [0,1,1] and [] respectively on S0, S1, S2. S0 is partitioned, S2 gets back and new leader should be elected between S1 and S2. In this case, S1 should be the only server elected because it has the complete logs. However S1 is on term 1 and S2 can be on term 100 and leader election will always fail without bring term up-to-date upon discovery. Implementation Raft Structure A recommended approach is to use different threads to handle LeaderElection, AppendEntries, ApplyStateMachine etc. It\u0026rsquo;s beneficial in both debugging and decoupling function call frequency. Most of the time, sleep in LeaderElection loop should longer than 2-3 times of AppendEntries RPC round trips to avoid too-frequent attempt of Leader Election. Sometimes the network package just drops and server doesn\u0026rsquo;t get response\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func (rf *Raft) MakeRaftInstance() { // initialize states go initLeaderElection() { for (raft_is_not_killed) { if (state == follower || state == candidate) \u0026amp;\u0026amp; electionTimeoutPassed { go attemptLeaderElection() } sleep((700 + jitter)ms) } } go initAppendEntries() { for (raft_is_not_killed) { if state == leader { go sendAppendEntries() } sleep(100ms) } } } RequestVoteRPC Election Timeout is the only thing determines whether a server should attempt Leader Election and Raft instance have a dedicate state for that. It\u0026rsquo;s very critical to reset Election Timeout in and only in 3 scenarios: candidate attempts Leader Election, follower receives AppendEntries RPC and follower grants a vote. Otherwise, likely Raft cluster will enter a livelock without any progress and you will never find it without serious debugging (Log every RPC request/response, grab a piece of paper, draw all the events in a chronological order, notice the cluster is always doing Leader Election from different servers without converge and have a head-scratcher\u0026hellip; like me spending couple hours :\u0026lt; ). A conceptual example can be illustrated below:\nS0, S1, S2, S3, S4 are having logs [0,1,2], [0,1,2], [], [], [] respectively. And suddenly S0 is partitioned The only server has the complete log is S1, however S1, S2, S3, S4 have equal probability to attempt Leader Election. And S1\u0026rsquo;s Leader Election is likely to be interrupted by other servers b\\c of split vote or higher term Resetting Election Timeout suppresses the Leader Election from in-eligible servers, for example S3, S4 and S1 has a lower chance to be interrupted and consequently higher chance to be elected. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 // Sender func (rf *Raft) attemptLeaderElection() { state = Candidate currentTerm += 1 votedFor = self reset_election_timeout votesReceivedCnt = 1 sendRequestCnt = 1 for follower in followers { go func() { ok := sendRequestVote(server, \u0026amp;args, \u0026amp;reply) if rpc_timeout_or_fail { return } if discover_higher_term { convert_to_follower } if receive_old_rpc { simply_discard } if receive_vote { votesReceivedCnt += 1 } sendRequestCnt += 1 }() } waitUntilEither(votesReceivedCnt \u0026gt;= len(followers)/2 or sendRequestCnt == len(followers)) if get_majority_votes { state = Leader go sendAppendEntries() } } // Receiver func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { if args.Term \u0026lt; rf.currentTerm { reject_vote return } /** Additional Safety constrains to determine whether a candidate can be elected **/ if args.Term \u0026gt; rf.currentTerm { approve_vote reset_election_timeout return } if args.Term == rf.currentTerm { approve_vote \u0026amp;\u0026amp; reset_election_timeout if neverVoted or votedFor == args.CandidateId else approve_vote return } } AppendEntriesRPC In Leader Election, AppendEntries RPC is like a heartbeat sending empty entries. The only thing is to reset Election Timeout on receiver\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // Sender func (rf *Raft) sendAppendEntries() { for follower in followers { go func() { ok := sendAppendEntries(server, \u0026amp;args, \u0026amp;reply) }() } } // Receiver func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { if args.Term \u0026lt; rf.currentTerm { reject_entries return } reset_election_timeout } Ref Raft Paper Suggestions on Raft Structure Raft Locking Advice ","date":"2025-12-16T00:00:00Z","permalink":"https://Gino-GouGe.github.io/blogs/p/raft-leader-election/","title":"Raft Leader Election"}]